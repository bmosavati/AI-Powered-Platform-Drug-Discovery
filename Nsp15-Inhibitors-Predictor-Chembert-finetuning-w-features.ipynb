{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmparsa\u001b[0m (\u001b[33mmypersonalteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6569d23e6fef4d1c96c9c11bff6228ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112743155616852, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/parsa/smiles_classification/wandb/run-20240421_224034-p3rd78l6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mypersonalteam/chemberta-smiles-binary-classification/runs/p3rd78l6/workspace' target=\"_blank\">fluent-dust-3</a></strong> to <a href='https://wandb.ai/mypersonalteam/chemberta-smiles-binary-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mypersonalteam/chemberta-smiles-binary-classification' target=\"_blank\">https://wandb.ai/mypersonalteam/chemberta-smiles-binary-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mypersonalteam/chemberta-smiles-binary-classification/runs/p3rd78l6/workspace' target=\"_blank\">https://wandb.ai/mypersonalteam/chemberta-smiles-binary-classification/runs/p3rd78l6/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import wandb\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"chemberta-smiles-binary-classification\")\n",
    "\n",
    "class ChemBERTaWithFeatures(nn.Module):\n",
    "    def __init__(self, chemberta_model_name, feature_dim):\n",
    "        super(ChemBERTaWithFeatures, self).__init__()\n",
    "        self.chemberta = AutoModel.from_pretrained(chemberta_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.feature_batch_norm = nn.BatchNorm1d(feature_dim)\n",
    "        self.classifier = nn.Linear(self.chemberta.config.hidden_size + feature_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, features):\n",
    "        chemberta_output = self.chemberta(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = chemberta_output.last_hidden_state[:, 0, :]\n",
    "        normalized_features = self.feature_batch_norm(features)\n",
    "        concatenated = torch.cat((cls_output, normalized_features), dim=1)\n",
    "        concatenated = self.dropout(concatenated)\n",
    "        logits = self.classifier(concatenated)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        return probabilities\n",
    "\n",
    "# Dataset class\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, encodings, features, labels):\n",
    "        self.encodings = encodings\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['features'] = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = preds.round()  # Convert probabilities to binary predictions\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, zero_division=0)  # Avoid division by zero\n",
    "    return accuracy, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parsa/conda/envs/p2/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.6817942215846136, Train Accuracy: 0.5689655172413793, Train Precision: 0.5786516853932584\n",
      "Epoch 1: Val Loss: 0.6392378211021423, Val Accuracy: 0.6, Val Precision: 0.5925925925925926\n",
      "Epoch 2: Train Loss: 0.6751704720350412, Train Accuracy: 0.6059113300492611, Train Precision: 0.6069651741293532\n",
      "Epoch 2: Val Loss: 0.633136585354805, Val Accuracy: 0.62, Val Precision: 0.5882352941176471\n",
      "Epoch 3: Train Loss: 0.6578912093089178, Train Accuracy: 0.6379310344827587, Train Precision: 0.6458333333333334\n",
      "Epoch 3: Val Loss: 0.6219532191753387, Val Accuracy: 0.7, Val Precision: 0.6785714285714286\n",
      "Epoch 4: Train Loss: 0.6474480330944061, Train Accuracy: 0.6896551724137931, Train Precision: 0.6896551724137931\n",
      "Epoch 4: Val Loss: 0.6149758249521255, Val Accuracy: 0.7, Val Precision: 0.6666666666666666\n",
      "Epoch 5: Train Loss: 0.6289966610761789, Train Accuracy: 0.7044334975369458, Train Precision: 0.7004830917874396\n",
      "Epoch 5: Val Loss: 0.6069946885108948, Val Accuracy: 0.7, Val Precision: 0.6785714285714286\n",
      "Epoch 6: Train Loss: 0.6070109812112955, Train Accuracy: 0.7093596059113301, Train Precision: 0.7033492822966507\n",
      "Epoch 6: Val Loss: 0.611212283372879, Val Accuracy: 0.64, Val Precision: 0.6129032258064516\n",
      "Epoch 7: Train Loss: 0.5862097579699296, Train Accuracy: 0.7216748768472906, Train Precision: 0.725\n",
      "Epoch 7: Val Loss: 0.6311145722866058, Val Accuracy: 0.66, Val Precision: 0.625\n",
      "Epoch 8: Train Loss: 0.5737745681634316, Train Accuracy: 0.7339901477832512, Train Precision: 0.7074235807860262\n",
      "Epoch 8: Val Loss: 0.6340222805738449, Val Accuracy: 0.64, Val Precision: 0.6060606060606061\n",
      "Epoch 9: Train Loss: 0.5606286858136837, Train Accuracy: 0.7438423645320197, Train Precision: 0.728110599078341\n",
      "Epoch 9: Val Loss: 0.6199179589748383, Val Accuracy: 0.64, Val Precision: 0.6060606060606061\n",
      "Epoch 10: Train Loss: 0.5263562638026017, Train Accuracy: 0.7487684729064039, Train Precision: 0.730593607305936\n",
      "Epoch 10: Val Loss: 0.6376830041408539, Val Accuracy: 0.62, Val Precision: 0.5882352941176471\n",
      "Epoch 11: Train Loss: 0.5333616745013458, Train Accuracy: 0.7610837438423645, Train Precision: 0.7523809523809524\n",
      "Epoch 11: Val Loss: 0.7257493361830711, Val Accuracy: 0.58, Val Precision: 0.5526315789473685\n",
      "Epoch 12: Train Loss: 0.5217478344073663, Train Accuracy: 0.7536945812807881, Train Precision: 0.7172995780590717\n",
      "Epoch 12: Val Loss: 0.6822198182344437, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 13: Train Loss: 0.4928145053294989, Train Accuracy: 0.7832512315270936, Train Precision: 0.7649769585253456\n",
      "Epoch 13: Val Loss: 0.6806842088699341, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 14: Train Loss: 0.4818820655345917, Train Accuracy: 0.7758620689655172, Train Precision: 0.7545454545454545\n",
      "Epoch 14: Val Loss: 0.7142882347106934, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 15: Train Loss: 0.455066883793244, Train Accuracy: 0.8103448275862069, Train Precision: 0.8088235294117647\n",
      "Epoch 15: Val Loss: 0.6917549669742584, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 16: Train Loss: 0.43975659287892854, Train Accuracy: 0.8201970443349754, Train Precision: 0.7954545454545454\n",
      "Epoch 16: Val Loss: 0.7615930438041687, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 17: Train Loss: 0.4303966027039748, Train Accuracy: 0.8275862068965517, Train Precision: 0.8093023255813954\n",
      "Epoch 17: Val Loss: 0.681605689227581, Val Accuracy: 0.56, Val Precision: 0.5454545454545454\n",
      "Epoch 18: Train Loss: 0.41896098450972485, Train Accuracy: 0.8251231527093597, Train Precision: 0.8113207547169812\n",
      "Epoch 18: Val Loss: 0.802291750907898, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 19: Train Loss: 0.4136129892789401, Train Accuracy: 0.8448275862068966, Train Precision: 0.8240740740740741\n",
      "Epoch 19: Val Loss: 0.8569105267524719, Val Accuracy: 0.64, Val Precision: 0.5853658536585366\n",
      "Epoch 20: Train Loss: 0.4147849277808116, Train Accuracy: 0.8300492610837439, Train Precision: 0.8130841121495327\n",
      "Epoch 20: Val Loss: 0.7956829369068146, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 21: Train Loss: 0.37652625430088776, Train Accuracy: 0.8669950738916257, Train Precision: 0.8743718592964824\n",
      "Epoch 21: Val Loss: 0.817984014749527, Val Accuracy: 0.66, Val Precision: 0.6\n",
      "Epoch 22: Train Loss: 0.3290916165480247, Train Accuracy: 0.8990147783251231, Train Precision: 0.8857142857142857\n",
      "Epoch 22: Val Loss: 0.7748197391629219, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 23: Train Loss: 0.33111874759197235, Train Accuracy: 0.8793103448275862, Train Precision: 0.8666666666666667\n",
      "Epoch 23: Val Loss: 0.8765519186854362, Val Accuracy: 0.62, Val Precision: 0.575\n",
      "Epoch 24: Train Loss: 0.32465505599975586, Train Accuracy: 0.8817733990147784, Train Precision: 0.8708133971291866\n",
      "Epoch 24: Val Loss: 0.8359336629509926, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 25: Train Loss: 0.2716796197570287, Train Accuracy: 0.9211822660098522, Train Precision: 0.9253731343283582\n",
      "Epoch 25: Val Loss: 0.7488294467329979, Val Accuracy: 0.62, Val Precision: 0.5789473684210527\n",
      "Epoch 26: Train Loss: 0.274051638176808, Train Accuracy: 0.8990147783251231, Train Precision: 0.8932038834951457\n",
      "Epoch 26: Val Loss: 0.8581640720367432, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 27: Train Loss: 0.2748447054853806, Train Accuracy: 0.9113300492610837, Train Precision: 0.9073170731707317\n",
      "Epoch 27: Val Loss: 0.8435226529836655, Val Accuracy: 0.58, Val Precision: 0.5625\n",
      "Epoch 28: Train Loss: 0.29515573439689785, Train Accuracy: 0.8916256157635468, Train Precision: 0.8955223880597015\n",
      "Epoch 28: Val Loss: 0.846837230026722, Val Accuracy: 0.6, Val Precision: 0.5757575757575758\n",
      "Epoch 29: Train Loss: 0.24564679654744956, Train Accuracy: 0.916256157635468, Train Precision: 0.9082125603864735\n",
      "Epoch 29: Val Loss: 0.9500545188784599, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 30: Train Loss: 0.22561223002580497, Train Accuracy: 0.9261083743842364, Train Precision: 0.9303482587064676\n",
      "Epoch 30: Val Loss: 1.0478069633245468, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 31: Train Loss: 0.2117495617041221, Train Accuracy: 0.9310344827586207, Train Precision: 0.9353233830845771\n",
      "Epoch 31: Val Loss: 1.0975552648305893, Val Accuracy: 0.64, Val Precision: 0.5897435897435898\n",
      "Epoch 32: Train Loss: 0.22867400428423515, Train Accuracy: 0.916256157635468, Train Precision: 0.9121951219512195\n",
      "Epoch 32: Val Loss: 1.1293903589248657, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 33: Train Loss: 0.18345834630040023, Train Accuracy: 0.9507389162561576, Train Precision: 0.9420289855072463\n",
      "Epoch 33: Val Loss: 1.146094650030136, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 34: Train Loss: 0.18813913945968336, Train Accuracy: 0.9408866995073891, Train Precision: 0.949748743718593\n",
      "Epoch 34: Val Loss: 1.1905687376856804, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 35: Train Loss: 0.1840170366832843, Train Accuracy: 0.9507389162561576, Train Precision: 0.9507389162561576\n",
      "Epoch 35: Val Loss: 1.1979315504431725, Val Accuracy: 0.58, Val Precision: 0.5588235294117647\n",
      "Epoch 36: Train Loss: 0.18590189043719035, Train Accuracy: 0.9334975369458128, Train Precision: 0.9356435643564357\n",
      "Epoch 36: Val Loss: 1.0962125808000565, Val Accuracy: 0.56, Val Precision: 0.5454545454545454\n",
      "Epoch 37: Train Loss: 0.1597195900976658, Train Accuracy: 0.9556650246305419, Train Precision: 0.964824120603015\n",
      "Epoch 37: Val Loss: 1.18754543364048, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 38: Train Loss: 0.15951063039784247, Train Accuracy: 0.9532019704433498, Train Precision: 0.9509803921568627\n",
      "Epoch 38: Val Loss: 1.3223042488098145, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 39: Train Loss: 0.13587540244826904, Train Accuracy: 0.9605911330049262, Train Precision: 0.9651741293532339\n",
      "Epoch 39: Val Loss: 1.273135244846344, Val Accuracy: 0.58, Val Precision: 0.5588235294117647\n",
      "Epoch 40: Train Loss: 0.17227126142153373, Train Accuracy: 0.9359605911330049, Train Precision: 0.927536231884058\n",
      "Epoch 40: Val Loss: 1.106234073638916, Val Accuracy: 0.56, Val Precision: 0.5454545454545454\n",
      "Epoch 41: Train Loss: 0.15087443819412819, Train Accuracy: 0.9532019704433498, Train Precision: 0.96\n",
      "Epoch 41: Val Loss: 1.3241641372442245, Val Accuracy: 0.58, Val Precision: 0.5588235294117647\n",
      "Epoch 42: Train Loss: 0.13173767308203074, Train Accuracy: 0.9605911330049262, Train Precision: 0.9605911330049262\n",
      "Epoch 42: Val Loss: 1.4005903154611588, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 43: Train Loss: 0.11098701349244668, Train Accuracy: 0.9753694581280788, Train Precision: 0.9800995024875622\n",
      "Epoch 43: Val Loss: 1.3930744528770447, Val Accuracy: 0.58, Val Precision: 0.5588235294117647\n",
      "Epoch 44: Train Loss: 0.1183326098208244, Train Accuracy: 0.9655172413793104, Train Precision: 0.9609756097560975\n",
      "Epoch 44: Val Loss: 1.3991710841655731, Val Accuracy: 0.58, Val Precision: 0.5588235294117647\n",
      "Epoch 45: Train Loss: 0.10914159322587344, Train Accuracy: 0.9630541871921182, Train Precision: 0.9795918367346939\n",
      "Epoch 45: Val Loss: 1.441173955798149, Val Accuracy: 0.64, Val Precision: 0.5945945945945946\n",
      "Epoch 46: Train Loss: 0.09717540586223969, Train Accuracy: 0.9753694581280788, Train Precision: 0.9753694581280788\n",
      "Epoch 46: Val Loss: 1.4286089688539505, Val Accuracy: 0.62, Val Precision: 0.5833333333333334\n",
      "Epoch 47: Train Loss: 0.1041119837990174, Train Accuracy: 0.9679802955665024, Train Precision: 0.9656862745098039\n",
      "Epoch 47: Val Loss: 1.4828841090202332, Val Accuracy: 0.62, Val Precision: 0.5833333333333334\n",
      "Epoch 48: Train Loss: 0.12189325948174183, Train Accuracy: 0.9630541871921182, Train Precision: 0.9607843137254902\n",
      "Epoch 48: Val Loss: 1.476311519742012, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 49: Train Loss: 0.1280024518760351, Train Accuracy: 0.9679802955665024, Train Precision: 0.9656862745098039\n",
      "Epoch 49: Val Loss: 1.520746573805809, Val Accuracy: 0.58, Val Precision: 0.5588235294117647\n",
      "Epoch 50: Train Loss: 0.08661694774547449, Train Accuracy: 0.9827586206896551, Train Precision: 0.9803921568627451\n",
      "Epoch 50: Val Loss: 1.4698143899440765, Val Accuracy: 0.58, Val Precision: 0.5588235294117647\n",
      "Epoch 51: Train Loss: 0.10255987003732187, Train Accuracy: 0.9679802955665024, Train Precision: 0.9702970297029703\n",
      "Epoch 51: Val Loss: 1.216644287109375, Val Accuracy: 0.62, Val Precision: 0.5833333333333334\n",
      "Epoch 52: Train Loss: 0.10348897756865391, Train Accuracy: 0.9679802955665024, Train Precision: 0.975\n",
      "Epoch 52: Val Loss: 1.2018844038248062, Val Accuracy: 0.62, Val Precision: 0.5833333333333334\n",
      "Epoch 53: Train Loss: 0.08464918421724668, Train Accuracy: 0.9729064039408867, Train Precision: 0.9615384615384616\n",
      "Epoch 53: Val Loss: 1.084180012345314, Val Accuracy: 0.6, Val Precision: 0.5757575757575758\n",
      "Epoch 54: Train Loss: 0.07690426657119623, Train Accuracy: 0.9827586206896551, Train Precision: 0.9851485148514851\n",
      "Epoch 54: Val Loss: 1.4974161088466644, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 55: Train Loss: 0.09356915033780612, Train Accuracy: 0.9753694581280788, Train Precision: 0.9753694581280788\n",
      "Epoch 55: Val Loss: 1.5016992092132568, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 56: Train Loss: 0.09628746902140287, Train Accuracy: 0.9753694581280788, Train Precision: 0.966183574879227\n",
      "Epoch 56: Val Loss: 1.2968186438083649, Val Accuracy: 0.58, Val Precision: 0.5625\n",
      "Epoch 57: Train Loss: 0.08409720902832654, Train Accuracy: 0.9729064039408867, Train Precision: 0.9705882352941176\n",
      "Epoch 57: Val Loss: 1.366778776049614, Val Accuracy: 0.56, Val Precision: 0.5454545454545454\n",
      "Epoch 58: Train Loss: 0.08367801973452935, Train Accuracy: 0.9802955665024631, Train Precision: 0.9802955665024631\n",
      "Epoch 58: Val Loss: 1.6962107568979263, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 59: Train Loss: 0.06144315212105329, Train Accuracy: 0.9827586206896551, Train Precision: 0.99\n",
      "Epoch 59: Val Loss: 1.7309995293617249, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 60: Train Loss: 0.09449632050326237, Train Accuracy: 0.9605911330049262, Train Precision: 0.9560975609756097\n",
      "Epoch 60: Val Loss: 1.771283894777298, Val Accuracy: 0.56, Val Precision: 0.5405405405405406\n",
      "Epoch 61: Train Loss: 0.09660319009652504, Train Accuracy: 0.9778325123152709, Train Precision: 0.9801980198019802\n",
      "Epoch 61: Val Loss: 1.7813915014266968, Val Accuracy: 0.56, Val Precision: 0.5405405405405406\n",
      "Epoch 62: Train Loss: 0.06970379752321886, Train Accuracy: 0.9778325123152709, Train Precision: 0.985\n",
      "Epoch 62: Val Loss: 1.7891127467155457, Val Accuracy: 0.58, Val Precision: 0.5526315789473685\n",
      "Epoch 63: Train Loss: 0.076681446391516, Train Accuracy: 0.9802955665024631, Train Precision: 0.9710144927536232\n",
      "Epoch 63: Val Loss: 1.7586132884025574, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 64: Train Loss: 0.05899796799684946, Train Accuracy: 0.9852216748768473, Train Precision: 0.9852216748768473\n",
      "Epoch 64: Val Loss: 1.7487813383340836, Val Accuracy: 0.62, Val Precision: 0.5789473684210527\n",
      "Epoch 65: Train Loss: 0.06704076766394652, Train Accuracy: 0.9827586206896551, Train Precision: 0.9851485148514851\n",
      "Epoch 65: Val Loss: 1.7578682750463486, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 66: Train Loss: 0.05619669698465329, Train Accuracy: 0.9901477832512315, Train Precision: 0.9950248756218906\n",
      "Epoch 66: Val Loss: 1.9511255025863647, Val Accuracy: 0.54, Val Precision: 0.5238095238095238\n",
      "Epoch 67: Train Loss: 0.08108722468694815, Train Accuracy: 0.9753694581280788, Train Precision: 0.9753694581280788\n",
      "Epoch 67: Val Loss: 1.763892412185669, Val Accuracy: 0.62, Val Precision: 0.5789473684210527\n",
      "Epoch 68: Train Loss: 0.06024048724569953, Train Accuracy: 0.9827586206896551, Train Precision: 0.9803921568627451\n",
      "Epoch 68: Val Loss: 1.692518413066864, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 69: Train Loss: 0.06744441189444982, Train Accuracy: 0.9778325123152709, Train Precision: 0.9801980198019802\n",
      "Epoch 69: Val Loss: 1.749846875667572, Val Accuracy: 0.6, Val Precision: 0.5714285714285714\n",
      "Epoch 70: Train Loss: 0.07914061822856848, Train Accuracy: 0.9679802955665024, Train Precision: 0.9656862745098039\n",
      "Epoch 70: Val Loss: 1.6689414530992508, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 71: Train Loss: 0.07897653491594471, Train Accuracy: 0.9802955665024631, Train Precision: 0.9850746268656716\n",
      "Epoch 71: Val Loss: 1.7528361082077026, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 72: Train Loss: 0.05436951179917042, Train Accuracy: 0.9802955665024631, Train Precision: 0.9850746268656716\n",
      "Epoch 72: Val Loss: 1.7799544036388397, Val Accuracy: 0.56, Val Precision: 0.5405405405405406\n",
      "Epoch 73: Train Loss: 0.06237303619631208, Train Accuracy: 0.9753694581280788, Train Precision: 0.9707317073170731\n",
      "Epoch 73: Val Loss: 1.6519624292850494, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 74: Train Loss: 0.030557255439746838, Train Accuracy: 0.9950738916256158, Train Precision: 0.9950738916256158\n",
      "Epoch 74: Val Loss: 1.6634569764137268, Val Accuracy: 0.62, Val Precision: 0.5789473684210527\n",
      "Epoch 75: Train Loss: 0.044546295064859666, Train Accuracy: 0.9852216748768473, Train Precision: 0.9900497512437811\n",
      "Epoch 75: Val Loss: 1.7506554275751114, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 76: Train Loss: 0.06433205288619949, Train Accuracy: 0.9802955665024631, Train Precision: 0.975609756097561\n",
      "Epoch 76: Val Loss: 1.5239185988903046, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 77: Train Loss: 0.04430785391909572, Train Accuracy: 0.9852216748768473, Train Precision: 0.9852216748768473\n",
      "Epoch 77: Val Loss: 1.6516970098018646, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 78: Train Loss: 0.033381460736004204, Train Accuracy: 0.9950738916256158, Train Precision: 0.9902439024390244\n",
      "Epoch 78: Val Loss: 1.7497163116931915, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 79: Train Loss: 0.029336682377526395, Train Accuracy: 0.9950738916256158, Train Precision: 0.9950738916256158\n",
      "Epoch 79: Val Loss: 1.7218179404735565, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 80: Train Loss: 0.01693399676766533, Train Accuracy: 1.0, Train Precision: 1.0\n",
      "Epoch 80: Val Loss: 1.8275182247161865, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 81: Train Loss: 0.03151376316180596, Train Accuracy: 0.9950738916256158, Train Precision: 0.9950738916256158\n",
      "Epoch 81: Val Loss: 1.4802067875862122, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 82: Train Loss: 0.058889552198636994, Train Accuracy: 0.9901477832512315, Train Precision: 0.9950248756218906\n",
      "Epoch 82: Val Loss: 1.9033012390136719, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 83: Train Loss: 0.04375867162329646, Train Accuracy: 0.9876847290640394, Train Precision: 0.9900990099009901\n",
      "Epoch 83: Val Loss: 2.0088901668787003, Val Accuracy: 0.54, Val Precision: 0.5277777777777778\n",
      "Epoch 84: Train Loss: 0.028261301573365927, Train Accuracy: 0.9901477832512315, Train Precision: 0.9901477832512315\n",
      "Epoch 84: Val Loss: 1.8285907804965973, Val Accuracy: 0.58, Val Precision: 0.5588235294117647\n",
      "Epoch 85: Train Loss: 0.027922431651789408, Train Accuracy: 0.9926108374384236, Train Precision: 1.0\n",
      "Epoch 85: Val Loss: 2.0068018436431885, Val Accuracy: 0.6, Val Precision: 0.5675675675675675\n",
      "Epoch 86: Train Loss: 0.049575422066622056, Train Accuracy: 0.9852216748768473, Train Precision: 0.9852216748768473\n",
      "Epoch 86: Val Loss: 1.9813207685947418, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 87: Train Loss: 0.038780442582300075, Train Accuracy: 0.9901477832512315, Train Precision: 0.9901477832512315\n",
      "Epoch 87: Val Loss: 1.917018324136734, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 88: Train Loss: 0.038331057159946516, Train Accuracy: 0.9926108374384236, Train Precision: 0.995049504950495\n",
      "Epoch 88: Val Loss: 2.0205723494291306, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 89: Train Loss: 0.03780518611893058, Train Accuracy: 0.9876847290640394, Train Precision: 0.9852941176470589\n",
      "Epoch 89: Val Loss: 2.0724215507507324, Val Accuracy: 0.56, Val Precision: 0.5405405405405406\n",
      "Epoch 90: Train Loss: 0.04655711288348986, Train Accuracy: 0.9901477832512315, Train Precision: 0.9853658536585366\n",
      "Epoch 90: Val Loss: 1.7982456684112549, Val Accuracy: 0.6, Val Precision: 0.5757575757575758\n",
      "Epoch 91: Train Loss: 0.04810254626835768, Train Accuracy: 0.9827586206896551, Train Precision: 0.9803921568627451\n",
      "Epoch 91: Val Loss: 1.979491800069809, Val Accuracy: 0.58, Val Precision: 0.5555555555555556\n",
      "Epoch 92: Train Loss: 0.03871837698926146, Train Accuracy: 0.9852216748768473, Train Precision: 0.9900497512437811\n",
      "Epoch 92: Val Loss: 2.0842630565166473, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 93: Train Loss: 0.02328832386634671, Train Accuracy: 0.9975369458128078, Train Precision: 0.9950980392156863\n",
      "Epoch 93: Val Loss: 2.102822571992874, Val Accuracy: 0.54, Val Precision: 0.5277777777777778\n",
      "Epoch 94: Train Loss: 0.027396663468187817, Train Accuracy: 0.9950738916256158, Train Precision: 0.9950738916256158\n",
      "Epoch 94: Val Loss: 1.565745733678341, Val Accuracy: 0.56, Val Precision: 0.5428571428571428\n",
      "Epoch 95: Train Loss: 0.03360594355931076, Train Accuracy: 0.9950738916256158, Train Precision: 1.0\n",
      "Epoch 95: Val Loss: 2.170832872390747, Val Accuracy: 0.54, Val Precision: 0.5277777777777778\n",
      "Epoch 96: Train Loss: 0.03078601253218949, Train Accuracy: 0.9926108374384236, Train Precision: 0.9901960784313726\n",
      "Epoch 96: Val Loss: 1.5164686609059572, Val Accuracy: 0.54, Val Precision: 0.53125\n",
      "Epoch 97: Train Loss: 0.0168411096629615, Train Accuracy: 1.0, Train Precision: 1.0\n",
      "Epoch 97: Val Loss: 1.5828474797308445, Val Accuracy: 0.54, Val Precision: 0.5294117647058824\n",
      "Epoch 98: Train Loss: 0.028845944710505698, Train Accuracy: 0.9950738916256158, Train Precision: 0.9950738916256158\n",
      "Epoch 98: Val Loss: 1.9810980260372162, Val Accuracy: 0.56, Val Precision: 0.5405405405405406\n",
      "Epoch 99: Train Loss: 0.0126335766309729, Train Accuracy: 0.9975369458128078, Train Precision: 1.0\n",
      "Epoch 99: Val Loss: 1.9177420735359192, Val Accuracy: 0.56, Val Precision: 0.5405405405405406\n",
      "Epoch 100: Train Loss: 0.02705465703128049, Train Accuracy: 0.9901477832512315, Train Precision: 0.9901477832512315\n",
      "Epoch 100: Val Loss: 1.7019798159599304, Val Accuracy: 0.56, Val Precision: 0.5405405405405406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data loading and preparation \n",
    "features_columns = ['Molecular Weight', 'LogP', 'Number of Atoms',\n",
    "       'Number of Bonds', 'Number of Rings', 'Rotatable Bonds Count',\n",
    "       'Hydrogen Bond Donors', 'Hydrogen Bond Acceptors',\n",
    "       'Number of Stereocenters', 'Topological Polar Surface Area (TPSA)']\n",
    "# labels = data['Results'].values\n",
    "\n",
    "  \n",
    "\n",
    "train_data = pd.read_csv('/home/parsa/smiles_classification/training_w_features.csv').sample(frac=1)\n",
    "val_data = pd.read_csv('/home/parsa/smiles_classification/validation_w_features.csv').sample(frac=1)\n",
    "\n",
    "X_train, X_val = train_data[features_columns].values, val_data[features_columns].values\n",
    "y_train, y_val = train_data['Results'].values, val_data['RESULT'].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DeepChem/ChemBERTa-77M-MLM')\n",
    "train_encodings = tokenizer(list(train_data['SMILES']), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(list(val_data['SMILES']), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = MoleculeDataset(train_encodings, X_train_scaled, y_train)\n",
    "val_dataset = MoleculeDataset(val_encodings, X_val_scaled, y_val)\n",
    "\n",
    "# Set up DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = ChemBERTaWithFeatures('DeepChem/ChemBERTa-77M-MLM', feature_dim=10).to(device)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, features)\n",
    "        loss = loss_function(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # wandb.log({\"train_loss\": total_loss / len(train_loader)})\n",
    "\n",
    "\n",
    "        all_preds.extend(outputs.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "    \n",
    "    train_accuracy, train_precision = compute_metrics(torch.tensor(all_preds), torch.tensor(all_labels))\n",
    "    wandb.log({\"train_loss\": total_loss / len(train_loader), \"train_accuracy\": train_accuracy, \"train_precision\": train_precision})\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask, features).squeeze(1)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(outputs.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    val_accuracy, val_precision = compute_metrics(torch.tensor(all_preds), torch.tensor(all_labels))\n",
    "    wandb.log({\"val_loss\": val_loss / len(val_loader), \"val_accuracy\": val_accuracy, \"val_precision\": val_precision})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {total_loss / len(train_loader)}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}\")\n",
    "    print(f\"Epoch {epoch+1}: Val Loss: {val_loss / len(val_loader)}, Val Accuracy: {val_accuracy}, Val Precision: {val_precision}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9494809210300446\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the validation set\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask, features)\n",
    "        loss = loss_function(outputs, labels.unsqueeze(1))\n",
    "        val_loss += loss.item()\n",
    "    wandb.log({\"val_loss\": val_loss / len(val_loader)})\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = train_df[['Molecular Weight', 'LogP', 'Number of Atoms',\n",
    "       'Number of Bonds', 'Number of Rings', 'Rotatable Bonds Count',\n",
    "       'Hydrogen Bond Donors', 'Hydrogen Bond Acceptors',\n",
    "       'Number of Stereocenters', 'Topological Polar Surface Area (TPSA)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3430e+02,  1.2444e+00,  1.7000e+01,  ...,  4.0000e+00,\n",
       "          1.0000e+00,  5.1020e+01],\n",
       "        [ 2.3634e+02,  2.1663e+00,  1.6000e+01,  ...,  3.0000e+00,\n",
       "          0.0000e+00,  3.3200e+01],\n",
       "        [ 2.7033e+02,  3.0282e+00,  2.0000e+01,  ...,  3.0000e+00,\n",
       "          0.0000e+00,  4.6340e+01],\n",
       "        ...,\n",
       "        [ 2.2332e+02,  1.6419e+00,  1.6000e+01,  ...,  2.0000e+00,\n",
       "          1.0000e+00,  4.9330e+01],\n",
       "        [ 2.0732e+02,  2.5996e+00,  1.5000e+01,  ...,  1.0000e+00,\n",
       "          2.0000e+00,  2.0310e+01],\n",
       "        [ 1.8222e+02, -9.0600e-02,  1.3000e+01,  ...,  2.0000e+00,\n",
       "          1.0000e+00,  4.9410e+01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(features_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4527],\n",
      "        [0.5496],\n",
      "        [0.4562],\n",
      "        [0.4339],\n",
      "        [0.5071],\n",
      "        [0.4529],\n",
      "        [0.5177],\n",
      "        [0.5702],\n",
      "        [0.4314],\n",
      "        [0.4699],\n",
      "        [0.4849],\n",
      "        [0.4800],\n",
      "        [0.5121],\n",
      "        [0.4870],\n",
      "        [0.5081],\n",
      "        [0.4606],\n",
      "        [0.4801],\n",
      "        [0.5726],\n",
      "        [0.5739],\n",
      "        [0.4679],\n",
      "        [0.5324],\n",
      "        [0.4366],\n",
      "        [0.5080],\n",
      "        [0.5031],\n",
      "        [0.5974],\n",
      "        [0.5608],\n",
      "        [0.5644],\n",
      "        [0.4994],\n",
      "        [0.5333],\n",
      "        [0.5753],\n",
      "        [0.5784],\n",
      "        [0.5474],\n",
      "        [0.5097],\n",
      "        [0.5337],\n",
      "        [0.4936],\n",
      "        [0.4952],\n",
      "        [0.5226],\n",
      "        [0.5336],\n",
      "        [0.5440],\n",
      "        [0.5430],\n",
      "        [0.4722],\n",
      "        [0.5452],\n",
      "        [0.5578],\n",
      "        [0.4671],\n",
      "        [0.5416],\n",
      "        [0.5158],\n",
      "        [0.5422],\n",
      "        [0.5308],\n",
      "        [0.4809],\n",
      "        [0.4390],\n",
      "        [0.5561],\n",
      "        [0.4472],\n",
      "        [0.4518],\n",
      "        [0.4788],\n",
      "        [0.5015],\n",
      "        [0.4888],\n",
      "        [0.5870],\n",
      "        [0.5219],\n",
      "        [0.4714],\n",
      "        [0.5442],\n",
      "        [0.5813],\n",
      "        [0.6344],\n",
      "        [0.5499],\n",
      "        [0.4388],\n",
      "        [0.4999],\n",
      "        [0.5509],\n",
      "        [0.5024],\n",
      "        [0.5055],\n",
      "        [0.5723],\n",
      "        [0.5305],\n",
      "        [0.4708],\n",
      "        [0.5865],\n",
      "        [0.5783],\n",
      "        [0.5903],\n",
      "        [0.5604],\n",
      "        [0.4608],\n",
      "        [0.5205],\n",
      "        [0.4954],\n",
      "        [0.6047],\n",
      "        [0.4863],\n",
      "        [0.4825],\n",
      "        [0.5032],\n",
      "        [0.4937],\n",
      "        [0.5006],\n",
      "        [0.4919],\n",
      "        [0.4547],\n",
      "        [0.4926],\n",
      "        [0.5375],\n",
      "        [0.5121],\n",
      "        [0.4997],\n",
      "        [0.4358],\n",
      "        [0.5930],\n",
      "        [0.5418],\n",
      "        [0.5084],\n",
      "        [0.6159],\n",
      "        [0.6354],\n",
      "        [0.4484],\n",
      "        [0.4696],\n",
      "        [0.5968],\n",
      "        [0.5660],\n",
      "        [0.6450],\n",
      "        [0.4554],\n",
      "        [0.5256],\n",
      "        [0.5068],\n",
      "        [0.5700],\n",
      "        [0.5192],\n",
      "        [0.5414],\n",
      "        [0.5396],\n",
      "        [0.5235],\n",
      "        [0.5736],\n",
      "        [0.5965],\n",
      "        [0.5702],\n",
      "        [0.5682],\n",
      "        [0.4647],\n",
      "        [0.5200],\n",
      "        [0.5304],\n",
      "        [0.6422],\n",
      "        [0.4900],\n",
      "        [0.5583],\n",
      "        [0.5786],\n",
      "        [0.5624],\n",
      "        [0.5693],\n",
      "        [0.5853],\n",
      "        [0.5330],\n",
      "        [0.4076],\n",
      "        [0.5246],\n",
      "        [0.4619],\n",
      "        [0.5704],\n",
      "        [0.4695],\n",
      "        [0.5186],\n",
      "        [0.4575],\n",
      "        [0.4947],\n",
      "        [0.5331],\n",
      "        [0.4342],\n",
      "        [0.4971],\n",
      "        [0.4799],\n",
      "        [0.5077],\n",
      "        [0.5016],\n",
      "        [0.4993],\n",
      "        [0.5479],\n",
      "        [0.4763],\n",
      "        [0.4960],\n",
      "        [0.5052],\n",
      "        [0.4349],\n",
      "        [0.5006],\n",
      "        [0.5318],\n",
      "        [0.5127],\n",
      "        [0.4995],\n",
      "        [0.5610],\n",
      "        [0.4618],\n",
      "        [0.4691],\n",
      "        [0.5765],\n",
      "        [0.5368],\n",
      "        [0.5422],\n",
      "        [0.4722],\n",
      "        [0.5447],\n",
      "        [0.6185],\n",
      "        [0.5016],\n",
      "        [0.5794],\n",
      "        [0.5353],\n",
      "        [0.5686],\n",
      "        [0.6378],\n",
      "        [0.5189],\n",
      "        [0.6289],\n",
      "        [0.5011],\n",
      "        [0.4732],\n",
      "        [0.5667],\n",
      "        [0.5144],\n",
      "        [0.5533],\n",
      "        [0.5948],\n",
      "        [0.5205],\n",
      "        [0.5773],\n",
      "        [0.5620],\n",
      "        [0.6235],\n",
      "        [0.5255],\n",
      "        [0.5125],\n",
      "        [0.5501],\n",
      "        [0.5596],\n",
      "        [0.5711],\n",
      "        [0.5117],\n",
      "        [0.4963],\n",
      "        [0.4963],\n",
      "        [0.5579],\n",
      "        [0.4848],\n",
      "        [0.5031],\n",
      "        [0.5383],\n",
      "        [0.4984],\n",
      "        [0.5829],\n",
      "        [0.6260],\n",
      "        [0.5453],\n",
      "        [0.5145],\n",
      "        [0.6287],\n",
      "        [0.5244],\n",
      "        [0.5023],\n",
      "        [0.5911],\n",
      "        [0.5470],\n",
      "        [0.5944],\n",
      "        [0.5713],\n",
      "        [0.5460],\n",
      "        [0.4836],\n",
      "        [0.4765],\n",
      "        [0.5527],\n",
      "        [0.5163],\n",
      "        [0.5523],\n",
      "        [0.4427],\n",
      "        [0.5974],\n",
      "        [0.4636],\n",
      "        [0.4474],\n",
      "        [0.4969],\n",
      "        [0.4866],\n",
      "        [0.5581],\n",
      "        [0.5435],\n",
      "        [0.4612],\n",
      "        [0.4450],\n",
      "        [0.5366],\n",
      "        [0.5753],\n",
      "        [0.5137],\n",
      "        [0.5274],\n",
      "        [0.5197],\n",
      "        [0.5809],\n",
      "        [0.5483],\n",
      "        [0.6116],\n",
      "        [0.5829],\n",
      "        [0.4929],\n",
      "        [0.4894],\n",
      "        [0.5500],\n",
      "        [0.4853],\n",
      "        [0.5271],\n",
      "        [0.5258],\n",
      "        [0.4549],\n",
      "        [0.5346],\n",
      "        [0.5004],\n",
      "        [0.5220],\n",
      "        [0.5006],\n",
      "        [0.4570],\n",
      "        [0.5343],\n",
      "        [0.4917],\n",
      "        [0.5979],\n",
      "        [0.6479],\n",
      "        [0.4612],\n",
      "        [0.5280],\n",
      "        [0.5828],\n",
      "        [0.5210],\n",
      "        [0.4190],\n",
      "        [0.5067],\n",
      "        [0.5256],\n",
      "        [0.4623],\n",
      "        [0.5657],\n",
      "        [0.5301],\n",
      "        [0.4979],\n",
      "        [0.6403],\n",
      "        [0.5525],\n",
      "        [0.4758],\n",
      "        [0.4714],\n",
      "        [0.5113],\n",
      "        [0.4496],\n",
      "        [0.4902],\n",
      "        [0.5854],\n",
      "        [0.5009],\n",
      "        [0.6140],\n",
      "        [0.4683],\n",
      "        [0.4597],\n",
      "        [0.5565],\n",
      "        [0.6208],\n",
      "        [0.5349],\n",
      "        [0.6695],\n",
      "        [0.4744],\n",
      "        [0.4973],\n",
      "        [0.5649],\n",
      "        [0.4965],\n",
      "        [0.5520],\n",
      "        [0.5016],\n",
      "        [0.5695],\n",
      "        [0.4918],\n",
      "        [0.5096],\n",
      "        [0.5094],\n",
      "        [0.4882],\n",
      "        [0.5283],\n",
      "        [0.4522],\n",
      "        [0.4526],\n",
      "        [0.4943],\n",
      "        [0.5043],\n",
      "        [0.5140],\n",
      "        [0.5221],\n",
      "        [0.5066],\n",
      "        [0.4708],\n",
      "        [0.5563],\n",
      "        [0.5167],\n",
      "        [0.5236],\n",
      "        [0.4990],\n",
      "        [0.5216],\n",
      "        [0.4982],\n",
      "        [0.4778],\n",
      "        [0.5176],\n",
      "        [0.4931],\n",
      "        [0.5229],\n",
      "        [0.5077],\n",
      "        [0.4932],\n",
      "        [0.5532],\n",
      "        [0.4840],\n",
      "        [0.5823],\n",
      "        [0.5151],\n",
      "        [0.4978],\n",
      "        [0.5217],\n",
      "        [0.4427],\n",
      "        [0.5362],\n",
      "        [0.5702],\n",
      "        [0.4794],\n",
      "        [0.5064],\n",
      "        [0.6251],\n",
      "        [0.5089],\n",
      "        [0.5813],\n",
      "        [0.4719],\n",
      "        [0.5059],\n",
      "        [0.5054],\n",
      "        [0.4980],\n",
      "        [0.4708],\n",
      "        [0.4604],\n",
      "        [0.4676],\n",
      "        [0.5006],\n",
      "        [0.5349],\n",
      "        [0.4607],\n",
      "        [0.4491],\n",
      "        [0.4700],\n",
      "        [0.4566],\n",
      "        [0.5475],\n",
      "        [0.4598],\n",
      "        [0.4466],\n",
      "        [0.5168],\n",
      "        [0.4878],\n",
      "        [0.4620],\n",
      "        [0.4917],\n",
      "        [0.5037],\n",
      "        [0.5087],\n",
      "        [0.5365],\n",
      "        [0.4768],\n",
      "        [0.5215],\n",
      "        [0.4903],\n",
      "        [0.5818],\n",
      "        [0.5522],\n",
      "        [0.4980],\n",
      "        [0.5586],\n",
      "        [0.4893],\n",
      "        [0.6014],\n",
      "        [0.4960],\n",
      "        [0.5866],\n",
      "        [0.5680],\n",
      "        [0.4810],\n",
      "        [0.5930],\n",
      "        [0.4713],\n",
      "        [0.5030],\n",
      "        [0.4927],\n",
      "        [0.6276],\n",
      "        [0.4518],\n",
      "        [0.5699],\n",
      "        [0.4858],\n",
      "        [0.4385],\n",
      "        [0.5795],\n",
      "        [0.6051],\n",
      "        [0.5039],\n",
      "        [0.4440],\n",
      "        [0.5109],\n",
      "        [0.5258],\n",
      "        [0.4573],\n",
      "        [0.5638],\n",
      "        [0.4638],\n",
      "        [0.4063],\n",
      "        [0.5513],\n",
      "        [0.5312],\n",
      "        [0.5475],\n",
      "        [0.4809],\n",
      "        [0.5771],\n",
      "        [0.5963],\n",
      "        [0.5766],\n",
      "        [0.5088],\n",
      "        [0.5375],\n",
      "        [0.4136],\n",
      "        [0.4626],\n",
      "        [0.5462],\n",
      "        [0.4785],\n",
      "        [0.5846],\n",
      "        [0.5103],\n",
      "        [0.5713],\n",
      "        [0.4374],\n",
      "        [0.4656],\n",
      "        [0.4514],\n",
      "        [0.5470],\n",
      "        [0.5316],\n",
      "        [0.4918],\n",
      "        [0.4716],\n",
      "        [0.5046],\n",
      "        [0.5345],\n",
      "        [0.5584],\n",
      "        [0.5816],\n",
      "        [0.5786],\n",
      "        [0.5555],\n",
      "        [0.4721],\n",
      "        [0.4099],\n",
      "        [0.5565],\n",
      "        [0.5260],\n",
      "        [0.5063],\n",
      "        [0.6422],\n",
      "        [0.5076],\n",
      "        [0.6370],\n",
      "        [0.5773],\n",
      "        [0.6024]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(train_df.SMILES.tolist(), return_tensors=\"pt\",padding=True)\n",
    "features = torch.tensor(features_df.values)\n",
    "\n",
    "# Normalize features (example, should use actual fitted scaler)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)  # This should be your training features\n",
    "normalized_features = torch.tensor(scaler.transform(features), dtype=torch.float32)\n",
    "\n",
    "# Forward pass\n",
    "probabilities = model(inputs['input_ids'], inputs['attention_mask'], normalized_features)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
